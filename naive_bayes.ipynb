{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to build a model using Naive Bayes to classify movie reviews into positive or negative, and then test the classifier on new movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "Our input for this problem are two groups of movie reviews, pos and neg, where each review is stored in a separate text file. The dataset can be downloaded from here: [movies_reviews](https://drive.google.com/file/d/1BzgcXlSRqFj1RoadBupx_OQm322-rHTt/view).\n",
    "\n",
    "The dataset is from the following publication: ''Thumbs up? Sentiment Classification using Machine Learning Techniques''. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Proceedings of EMNLP, pp. 79--86, 2002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read files into arrays \n",
    "We first read files and store their content into 2 string arrays, where each element is a string of one movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"../data_set/movies_reviews\"\n",
    "pos_entries = os.listdir(data_dir+\"/pos\")\n",
    "neg_entries = os.listdir(data_dir+\"/neg\")\n",
    "\n",
    "pos_ls = []\n",
    "for file in pos_entries:\n",
    "    file = data_dir + \"/pos/\" + file\n",
    "    text = open(file,\"r\").read()\n",
    "    pos_ls.append(text)\n",
    "    \n",
    "neg_ls = []\n",
    "for file in neg_entries:\n",
    "    file = data_dir + \"/neg/\" + file\n",
    "    text = open(file,\"r\").read()\n",
    "    neg_ls.append(text)\n",
    "    \n",
    "len(pos_ls),len(neg_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vectorization of documents\n",
    "To convert the words in each document into a vector of word occurrences, we use use a special class from <code>sklearn.feature_extraction.text</code> called <code>CountVectorizer</code>. It works by first tokenizing, i.e, assigning the unique numbers to each of the words in a text, and then counting the occurrence of these numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the vectorization, we need to first find out the stopped word that are very commonly used in all documents no matter the class of the document, given in file \"stop_words.txt\". For example, \"the\" appears a lot in both negative and positive movie reviews. These words would interfere with our classification result so we need to remove them before do anything further. To do that, we can pass it as a parameter to <code>CountVectorizer</code> to remove the stopped words from the tokens in all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "stop_words_file = \"../data_set/movies_reviews/stop_words.txt\"\n",
    "f = open(stop_words_file, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "stopwords = []\n",
    "for line in f:\n",
    "    stopwords.append(line.strip())\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of count is: (2005, 39373)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector = CountVectorizer(stop_words=stopwords)\n",
    "text_ls = pos_ls+neg_ls\n",
    "vector.fit(text_ls)\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "voc_dic = vector.vocabulary_\n",
    "\n",
    "# produce counts of occurrences of each word in each document\n",
    "counts = vector.transform(text_ls)\n",
    "print(\"The shape of count is: \"+str(counts.shape))\n",
    "counts = counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add class labels and perform train test split\n",
    "We can add the labels pos or neg to the vector depending on the directory of the movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2005,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1-1005 are positive reviews so our label is 1\n",
    "Y_orig = np.ones((1005,)) \n",
    "\n",
    "# 1005-2005 are negative reviews so our label is 0\n",
    "Y_orig = np.concatenate((Y_orig, np.zeros((1000,)))) \n",
    "Y = Y_orig.reshape(-1)\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, Y, test_size=0.3,random_state=109)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classify with Naive Bayse Model\n",
    "We use multinomial model in sklearn library because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test accuracy and predict new reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result shows that..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
